{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KgkZ7bbBELJ"
   },
   "source": [
    "## Configuration générale de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7742,
     "status": "ok",
     "timestamp": 1729137830980,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "M18NBNNiAY36",
    "outputId": "4ff597bd-cc23-4d05-c263-19737ce9fd67"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "wd = \"/content/drive/MyDrive/manifold\"\n",
    "\n",
    "import os\n",
    "# fixer le repertoire de travail\n",
    "os.chdir(wd)\n",
    "# Forcer UTF-8\n",
    "os.environ['LC_ALL'] = 'C.UTF-8'\n",
    "os.environ['LANG'] = 'C.UTF-8'\n",
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# Installation optuna ( recalcitrante par pip)\n",
    "import subprocess\n",
    "subprocess.run(['pip', 'install', 'optuna'])\n",
    "\n",
    "\n",
    "# Lister les GPU disponibles et leurs carctéristiques\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs disponibles : {len(gpus)}\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i}: {gpu}\")\n",
    "else:\n",
    "    print(\"Aucun GPU disponible.\")\n",
    "\n",
    "for gpu in gpus:\n",
    "    details = tf.config.experimental.get_device_details(gpu)\n",
    "    print(details)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzItQ8nmEHuZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729137834232,
     "user_tz": -120,
     "elapsed": 3254,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     }
    },
    "outputId": "51acbae0-948d-443e-f5d9-bcf6d1ce92db"
   },
   "source": [
    "# Bibilothèque \"rapids\" pour accélérer le calcul matriciel\n",
    "!pip install\\\n",
    "    --extra-index-url=https://pypi.nvidia.com \\\n",
    "    cudf-cu12==24.10.* dask-cudf-cu12==24.10.* cuml-cu12==24.10.* \\\n",
    "    cugraph-cu12==24.10.* cuspatial-cu12==24.10.* cuproj-cu12==24.10.* \\\n",
    "    cuxfilter-cu12==24.10.* cucim-cu12==24.10.* pylibraft-cu12==24.10.* \\\n",
    "    raft-dask-cu12==24.10.* cuvs-cu12==24.10.* nx-cugraph-cu12==24.10.*"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6rlnz0PBFJd"
   },
   "source": [
    "## Configuration locale en termes de librairies et d'outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SZcVnrEjCqzF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729137837213,
     "user_tz": -120,
     "elapsed": 2984,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     }
    }
   },
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import cuml\n",
    "from cuml.linear_model import LogisticRegression as cuML_LogReg\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import optuna\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Dense, Flatten, Reshape, BatchNormalization, LeakyReLU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Dense, Flatten, Reshape, BatchNormalization, LeakyReLU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-ph5xsdCroh"
   },
   "source": [
    "# Travail sur le dataset Fashion_mnist\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF4CXtxVLIBi"
   },
   "source": [
    "## 1. Présentation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1729137838084,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "5sQkGtjpBFMh",
    "outputId": "5ef16609-9705-4558-85aa-5a3db03967a7"
   },
   "source": [
    "# Charger le dataset Fashion-MNIST\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Fusionner les ensembles train et test pour une vue d'ensemble\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# Redimensionner les images\n",
    "X_reshaped = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Informations générales sur le dataset\n",
    "def dataset_overview():\n",
    "    print(\"=== Aperçu du Dataset Fashion-MNIST ===\")\n",
    "    print(f\"Nombre total d'exemples : {X.shape[0]}\")\n",
    "    print(f\"Nombre total de caractéristiques (pixels) : {X_reshaped.shape[1]}\")\n",
    "    print(f\"Dimensions des images : {X.shape[1]}x{X.shape[2]}\")\n",
    "    print(f\"Nombre de classes : {len(np.unique(y))}\")\n",
    "    print(\"\\nExemples de classes :\")\n",
    "    print(np.unique(y))\n",
    "\n",
    "# Vérification des valeurs manquantes\n",
    "def check_missing_values():\n",
    "    missing_values = np.isnan(X).sum()\n",
    "    print(f\"\\n=== Valeurs manquantes ===\")\n",
    "    print(f\"Nombre total de valeurs manquantes : {missing_values}\")\n",
    "\n",
    "# Distribution des classes\n",
    "def class_distribution():\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_dist = dict(zip(unique, counts))\n",
    "    print(\"\\n=== Distribution des classes ===\")\n",
    "    for key, value in class_dist.items():\n",
    "        print(f\"Classe {key} : {value} images\")\n",
    "\n",
    "# Affichage d'exemples d'images\n",
    "def show_sample_images():\n",
    "    print(\"\\n=== Affichage de quelques exemples d'images ===\")\n",
    "    fig, ax = plt.subplots(2, 5, figsize=(10, 5))\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        axi.imshow(X[i], cmap='gray')\n",
    "        axi.set(xticks=[], yticks=[], xlabel=f'Label: {y[i]}')\n",
    "    plt.suptitle(\"Exemples d'images du Dataset Fashion-MNIST\")\n",
    "    plt.show()\n",
    "\n",
    "# Fonction principale pour présenter le dataset à l'examinateur\n",
    "def present_dataset():\n",
    "    dataset_overview()\n",
    "    check_missing_values()\n",
    "    class_distribution()\n",
    "    show_sample_images()\n",
    "\n",
    "# Appeler la fonction pour présenter le dataset\n",
    "present_dataset()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEHiQ_TqL9hj"
   },
   "source": [
    "## 2. Calibrer les hyperparamètres\n",
    "\n",
    "Nous nous servirons également de la librairie \"Optuna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OfYXRgaBvG_4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729139954798,
     "user_tz": -120,
     "elapsed": 2116718,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     }
    },
    "outputId": "456fcfde-ee2e-4dc2-f7ff-3f11d6850304"
   },
   "source": [
    "def create_autoencoder(trial):\n",
    "    # Hyperparamètres à optimiser\n",
    "    num_filters_1 = trial.suggest_int('num_filters_1', 32, 128)\n",
    "    num_filters_2 = trial.suggest_int('num_filters_2', 64, 256)\n",
    "    latent_dim = trial.suggest_int('latent_dim', 32, 128)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "\n",
    "    # Encodeur\n",
    "    input_img = Input(shape=(28, 28, 1))\n",
    "    x = Conv2D(num_filters_1, (3, 3), padding='same')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Conv2D(num_filters_2, (3, 3), padding='same', strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    encoded = Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "    # Décodeur\n",
    "    x = Dense(14 * 14 * num_filters_2, activation='relu')(encoded)\n",
    "    x = Reshape((14, 14, num_filters_2))(x)\n",
    "    x = Conv2DTranspose(num_filters_2, (3, 3), padding='same', strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Conv2DTranspose(num_filters_1, (3, 3), padding='same')(x)\n",
    "    decoded = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    autoencoder.compile(optimizer=optimizer, loss=MeanSquaredError())\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "def objective(trial):\n",
    "    # Chargement des données\n",
    "    (X_train, _), (X_test, _) = fashion_mnist.load_data()\n",
    "    X_train = X_train.astype('float32') / 255.\n",
    "    X_test = X_test.astype('float32') / 255.\n",
    "    X_train = X_train.reshape((len(X_train), 28, 28, 1))\n",
    "    X_test = X_test.reshape((len(X_test), 28, 28, 1))\n",
    "\n",
    "    # Création du modèle\n",
    "    model = create_autoencoder(trial)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    model.fit(X_train, X_train, epochs=5, batch_size=128, validation_data=(X_test, X_test), verbose=0)\n",
    "\n",
    "    # Évaluation sur les données de test (perte MSE)\n",
    "    loss = model.evaluate(X_test, X_test, verbose=0)\n",
    "    return loss\n",
    "\n",
    "# Création de l'étude Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Affichage des meilleurs hyperparamètres\n",
    "print(\"Meilleurs hyperparamètres : \", study.best_params)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nbzt7DtmvHC7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729139954798,
     "user_tz": -120,
     "elapsed": 16,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     }
    }
   },
   "source": [
    "# Récupérer Les meilleurs hyperparamètres\n",
    "best_params = study.best_params\n",
    "# Enregistrer les meilleurs hyperparamètres dans un fichier JSON dans ./results\n",
    "import json\n",
    "with open('./results/best_params_encoders_fashion_mnist.json', 'w') as f:\n",
    "    json.dump(best_params, f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xtzQnUC0vHFw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729139954798,
     "user_tz": -120,
     "elapsed": 6,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     }
    }
   },
   "source": [
    "# Charger les meilleurs hyperparamètres à partir du fichier JSON dans ./results\n",
    "import json\n",
    "with open('./results/best_params_encoders_fashion_mnist.json', 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# ( Ces étapes sont pour éviter de rejouer le calibrage si interruption de session)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR9wc_z5vHJO"
   },
   "source": [
    "## 3. Modèle Auto encoder avec les best_params évalués"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XYquNx7T6ebJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1729139954798,
     "user_tz": -120,
     "elapsed": 5,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     }
    }
   },
   "source": [
    "# Fonction pour créer l'autoencodeur avec les hyperparamètres optimisés\n",
    "def create_autoencoder_with_best_params(best_params):\n",
    "    num_filters_1 = best_params['num_filters_1']\n",
    "    num_filters_2 = best_params['num_filters_2']\n",
    "    latent_dim = best_params['latent_dim']\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "\n",
    "    # Encodeur\n",
    "    input_img = Input(shape=(28, 28, 1))\n",
    "    x = Conv2D(num_filters_1, (3, 3), padding='same')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Conv2D(num_filters_2, (3, 3), padding='same', strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    encoded = Dense(latent_dim, activation='relu')(x)  # Espace latent\n",
    "\n",
    "    # Décodeur\n",
    "    x = Dense(14 * 14 * num_filters_2, activation='relu')(encoded)\n",
    "    x = Reshape((14, 14, num_filters_2))(x)\n",
    "    x = Conv2DTranspose(num_filters_2, (3, 3), padding='same', strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Conv2DTranspose(num_filters_1, (3, 3), padding='same')(x)\n",
    "    decoded = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    # Modèle autoencodeur complet\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # Compilation du modèle\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    autoencoder.compile(optimizer=optimizer, loss=MeanSquaredError())\n",
    "\n",
    "    # Renvoie à la fois l'autoencodeur complet et l'encodeur seul\n",
    "    encoder = Model(input_img, encoded)  # Modèle de l'encodeur (pour espace latent)\n",
    "\n",
    "    return autoencoder, encoder\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150808,
     "status": "ok",
     "timestamp": 1729140105601,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "_4c6eKQ872_E",
    "outputId": "a6ff65c8-2afc-490e-d47b-ed3effe23015"
   },
   "source": [
    "# Chargement des données Fashion MNIST\n",
    "(X_train, _), (X_test, _) = fashion_mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "X_train = X_train.reshape((len(X_train), 28, 28, 1))\n",
    "X_test = X_test.reshape((len(X_test), 28, 28, 1))\n",
    "\n",
    "# Création de l'autoencodeur et de l'encodeur\n",
    "autoencoder, encoder = create_autoencoder_with_best_params(best_params)\n",
    "\n",
    "# Entraînement de l'autoencodeur\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=128, validation_data=(X_test, X_test))\n",
    "\n",
    "# Utilisation de l'encodeur pour obtenir les représentations dans l'espace latent\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "# Affichage des représentations encodées\n",
    "print(\"Représentations encodées de X_train : \", X_train_encoded.shape)\n",
    "print(\"Représentations encodées de X_test : \", X_test_encoded.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6MJpOmj-iJ6"
   },
   "source": [
    "## 4. Evaluation de la qualité de la reduction de dimension obtenue\n",
    "\n",
    "### 4.1 Visualistion de l'information (classes) avec t-sne 2d original vs réduit\n",
    "\n",
    "#### En espace réduit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 26549,
     "status": "ok",
     "timestamp": 1729140132135,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "4lr2o0tf73O4",
    "outputId": "baac266a-b461-49b6-b89e-2d347459cc27"
   },
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Réduction de la dimension avec t-SNE pour visualisation\n",
    "tsne = TSNE(n_components=2)\n",
    "X_test_encoded_tsne = tsne.fit_transform(X_test_encoded)\n",
    "\n",
    "# Nom des classes de Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Plot des données encodées projetées dans un espace 2D\n",
    "scatter = plt.scatter(X_test_encoded_tsne[:, 0], X_test_encoded_tsne[:, 1], c=_, cmap='tab10')\n",
    "\n",
    "# Ajouter une barre de couleur avec les noms des classes\n",
    "cbar = plt.colorbar(scatter, ticks=np.arange(10))  # Ajoute des ticks pour 10 classes\n",
    "cbar.ax.set_yticklabels(class_names)  # Remplace les ticks par les noms des classes\n",
    "\n",
    "# Ajouter un titre\n",
    "plt.title(\"Visualisation des représentations encodées avec t-SNE\")\n",
    "\n",
    "# Afficher le plot\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 26102,
     "status": "ok",
     "timestamp": 1729140158234,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "0uKXODy6NfMc",
    "outputId": "9d598377-5313-4490-abf7-539ce9572fad"
   },
   "source": [
    "#### En espace d'origine\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "X_test_flat = X_test.reshape(-1, 28 * 28)\n",
    "\n",
    "# Réduction de la dimension avec t-SNE pour visualisation\n",
    "tsne = TSNE(n_components=2)\n",
    "X_test_flat_tsne = tsne.fit_transform(X_test_flat)\n",
    "\n",
    "# Nom des classes de Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "# Plot des données encodées projetées dans un espace 2D\n",
    "scatter = plt.scatter(X_test_flat_tsne[:, 0], X_test_flat_tsne[:, 1], c=_, cmap='tab10')\n",
    "\n",
    "# Ajouter une barre de couleur avec les noms des classes\n",
    "cbar = plt.colorbar(scatter, ticks=np.arange(10))  # Ajoute des ticks pour 10 classes\n",
    "cbar.ax.set_yticklabels(class_names)  # Remplace les ticks par les noms des classes\n",
    "\n",
    "# Ajouter un titre\n",
    "plt.title(\"Visualisation des représentations d'origine avec t-SNE\")\n",
    "\n",
    "# Afficher le plot\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGlwr4SeNfVO"
   },
   "source": [
    "### 4.2 PCA : espace d'origine vs réduit"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualisation de l'information en espace réduit avec PCA 2d"
   ],
   "metadata": {
    "id": "otHoHJb5-rFE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1729140158827,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "fq7HooSU73ZF",
    "outputId": "14082154-c1f4-4f44-f1dd-73e3c062b48d"
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Réduction de la dimension à 2 avec PCA pour visualisation\n",
    "pca = PCA(n_components=2)\n",
    "X_test_encoded_pca = pca.fit_transform(X_test_encoded)\n",
    "\n",
    "# Nom des classes de Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Utiliser les labels de Fashion MNIST (remplacer _ par y_test)\n",
    "y_test_subset = _  # Exemple avec tous les labels de test\n",
    "\n",
    "# Plot des données encodées projetées dans un espace 2D\n",
    "scatter = plt.scatter(X_test_encoded_pca[:, 0], X_test_encoded_pca[:, 1], c=y_test_subset, cmap='tab10')\n",
    "\n",
    "# Ajouter une barre de couleur avec les noms des classes\n",
    "cbar = plt.colorbar(scatter, ticks=np.arange(10))  # Ajoute des ticks pour 10 classes\n",
    "cbar.ax.set_yticklabels(class_names)  # Remplace les ticks par les noms des classes\n",
    "\n",
    "# Ajouter un titre\n",
    "plt.title(\"Visualisation des représentations encodées avec PCA\")\n",
    "\n",
    "# Afficher le plot\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr0Ve-mf73i6"
   },
   "source": [
    "### En espace réduit en 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "executionInfo": {
     "elapsed": 1752,
     "status": "ok",
     "timestamp": 1729140160574,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "msygRpK273rt",
    "outputId": "29cd7247-e20f-4672-dc95-a1f549a6b9cc"
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import pour visualisation 3D\n",
    "import numpy as np\n",
    "\n",
    "# Réduction de la dimension à 3 avec PCA pour visualisation\n",
    "pca = PCA(n_components=3)\n",
    "X_test_encoded_pca_3d = pca.fit_transform(X_test_encoded)\n",
    "\n",
    "# Nom des classes de Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Utiliser les labels de Fashion MNIST (remplacer _ par y_test)\n",
    "y_test_subset = _  # Remplacer par y_test si disponible\n",
    "\n",
    "# Visualisation en 3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot des données encodées projetées dans un espace 3D\n",
    "sc = ax.scatter(X_test_encoded_pca_3d[:, 0], X_test_encoded_pca_3d[:, 1], X_test_encoded_pca_3d[:, 2], c=y_test_subset, cmap='tab10')\n",
    "\n",
    "# Ajouter une barre de couleur avec les noms des classes\n",
    "cbar = plt.colorbar(sc, ticks=np.arange(10))  # Ajoute des ticks pour les 10 classes\n",
    "cbar.ax.set_yticklabels(class_names)  # Remplace les ticks numériques par les noms des classes\n",
    "\n",
    "# Ajouter les titres des axes\n",
    "ax.set_xlabel('Composante 1')\n",
    "ax.set_ylabel('Composante 2')\n",
    "ax.set_zlabel('Composante 3')\n",
    "\n",
    "# Ajouter un titre\n",
    "ax.set_title(\"PCA latent\")\n",
    "\n",
    "# Afficher le graphe\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDYyXwGFQAUe"
   },
   "source": [
    "### En espace d 'origine en 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1729140161152,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "RsmVBXv4730J",
    "outputId": "160ae340-ac63-449f-8168-d505b96335db"
   },
   "source": [
    "\n",
    "\n",
    "# Aplatir les images de 28x28 pixels en vecteurs de taille 784\n",
    "X_test_flat = X_test.reshape(-1, 28*28)\n",
    "\n",
    "# Réduction de la dimension à 3 avec PCA pour visualisation\n",
    "pca = PCA(n_components=3)\n",
    "X_test_pca_3d = pca.fit_transform(X_test_flat)\n",
    "\n",
    "# Nom des classes de Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Visualisation en 3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot des données brutes projetées dans un espace 3D avec PCA\n",
    "sc = ax.scatter(X_test_pca_3d[:, 0], X_test_pca_3d[:, 1], X_test_pca_3d[:, 2], c=y_test, cmap='tab10')\n",
    "\n",
    "# Ajouter une barre de couleur avec les noms des classes\n",
    "cbar = plt.colorbar(sc, ticks=np.arange(10))  # Ajoute des ticks pour les 10 classes\n",
    "cbar.ax.set_yticklabels(class_names)  # Remplace les ticks numériques par les noms des classes\n",
    "\n",
    "# Ajouter les titres des axes\n",
    "ax.set_xlabel('Composante 1')\n",
    "ax.set_ylabel('Composante 2')\n",
    "ax.set_zlabel('Composante 3')\n",
    "\n",
    "# Ajouter un titre\n",
    "ax.set_title(\"PCA origine\")\n",
    "\n",
    "# Afficher le graphe\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "executionInfo": {
     "elapsed": 2482,
     "status": "ok",
     "timestamp": 1729140163628,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "qvv4ueJd739S",
    "outputId": "6b02179d-fa1c-437f-bb49-81d81c2d43e0"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prédire les reconstructions sur les données de test\n",
    "X_test_reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# Calcul de l'erreur de reconstruction (MSE)\n",
    "mse = mean_squared_error(X_test.flatten(), X_test_reconstructed.flatten())\n",
    "print(f\"Erreur de reconstruction (MSE) : {mse}\")\n",
    "\n",
    "# Nombre d'exemples à afficher\n",
    "num_examples = 4\n",
    "\n",
    "# Créer une figure pour afficher les images\n",
    "fig, axes = plt.subplots(2, num_examples, figsize=(15, 6))\n",
    "\n",
    "# Visualiser les images originales et reconstruites\n",
    "for i in range(num_examples):\n",
    "    # Afficher l'image originale\n",
    "    axes[0, i].imshow(X_test[i].reshape(28, 28), cmap='gray')  # Adapter la forme si les images ne sont pas 28x28\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title(\"Original\")\n",
    "\n",
    "    # Afficher l'image reconstruite\n",
    "    axes[1, i].imshow(X_test_reconstructed[i].reshape(28, 28), cmap='gray')  # Adapter la forme si nécessaire\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title(\"Reconstruite\")\n",
    "\n",
    "# Afficher les images\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ypHpy_ZGg2-"
   },
   "source": [
    "### Comparatif de prédiction sur l'espace latent et l'espace d'origine d'un mêm algo\n",
    "\n",
    "Il s'agira ici de comparer la pertinence de l'information gardée, en essayant d'en extraire les labels avec le même algo : une regression logistique multinomiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5805,
     "status": "ok",
     "timestamp": 1729140169429,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "9PANlau-74Fu",
    "outputId": "a8cee1a9-45a4-4dc9-ae60-6b38e28c2fcd"
   },
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cuml import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Aplatir les images de 28x28 en vecteurs de 784 dimensions pour l'espace d'origine\n",
    "X_train_flat = X_train.reshape(-1, 28*28).astype(np.float32)\n",
    "X_test_flat = X_test.reshape(-1, 28*28).astype(np.float32)\n",
    "\n",
    "# Partie 1: Régression logistique sur l'espace d'origine (784 dimensions)\n",
    "# Passer les données à cuML (GPU) en utilisant cupy\n",
    "X_train_flat_gpu = cp.asarray(X_train_flat)\n",
    "X_test_flat_gpu = cp.asarray(X_test_flat)\n",
    "y_train_gpu = cp.asarray(y_train)\n",
    "y_test_gpu = cp.asarray(y_test)\n",
    "\n",
    "# Entraîner une régression logistique sur l'espace d'origine\n",
    "logreg_origin = LogisticRegression(max_iter=500)\n",
    "logreg_origin.fit(X_train_flat_gpu, y_train_gpu)\n",
    "\n",
    "# Prédictions sur l'espace d'origine\n",
    "y_pred_origin = logreg_origin.predict(X_test_flat_gpu)\n",
    "\n",
    "# Calcul des métriques sur l'espace d'origine\n",
    "accuracy_origin = accuracy_score(cp.asnumpy(y_test_gpu), cp.asnumpy(y_pred_origin))\n",
    "precision_origin = precision_score(cp.asnumpy(y_test_gpu), cp.asnumpy(y_pred_origin), average='weighted')\n",
    "recall_origin = recall_score(cp.asnumpy(y_test_gpu), cp.asnumpy(y_pred_origin), average='weighted')\n",
    "f1_origin = f1_score(cp.asnumpy(y_test_gpu), cp.asnumpy(y_pred_origin), average='weighted')\n",
    "\n",
    "# Partie 2: Régression logistique sur l'espace latent (réduit avec l'autoencodeur)\n",
    "# Utiliser l'encodeur pour obtenir les représentations dans l'espace latent\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "# Normaliser les données de l'espace latent\n",
    "scaler = StandardScaler()\n",
    "X_train_encoded_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_encoded_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# Convertir en GPU arrays pour cuML\n",
    "X_train_encoded_gpu = cp.asarray(X_train_encoded_scaled)\n",
    "X_test_encoded_gpu = cp.asarray(X_test_encoded_scaled)\n",
    "\n",
    "# Entraîner une régression logistique sur l'espace latent\n",
    "logreg_latent = LogisticRegression(max_iter=500)  # Augmentation du nombre d'itérations\n",
    "logreg_latent.fit(X_train_encoded_gpu, y_train_gpu)\n",
    "\n",
    "# Prédictions sur l'espace latent\n",
    "y_pred_latent = logreg_latent.predict(X_test_encoded_gpu)\n",
    "\n",
    "# Calcul des métriques sur l'espace latent\n",
    "accuracy_latent = accuracy_score(cp.asnumpy(y_test_gpu), cp.asnumpy(y_pred_latent))\n",
    "precision_latent = precision_score(cp.asnumpy(y_test_gpu), cp.asnumpy(y_pred_latent), average='weighted')\n",
    "recall_latent = recall_score(cp.asnumpy(y_test_gpu), cp.asnumpy(y_pred_latent), average='weighted')\n",
    "f1_latent = f1_score(cp.asnumpy(y_test_gpu), cp.asnumpy(y_pred_latent), average='weighted')\n",
    "\n",
    "# Récapitulatif des métriques\n",
    "summary = {\n",
    "    'Métrique': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Origine': [accuracy_origin, precision_origin, recall_origin, f1_origin],\n",
    "    'Latent': [accuracy_latent, precision_latent, recall_latent, f1_latent]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(summary_df)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37P0Bs3eJzIe"
   },
   "source": [
    "## Pour tester : visualiser la representation abstraite des images en espace réduit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1729140170415,
     "user": {
      "displayName": "m.m m",
      "userId": "09859000430014588739"
     },
     "user_tz": -120
    },
    "id": "DsI6y3nk74NO",
    "outputId": "3b955709-0032-4fb2-f91b-a330cf4f6ab2"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prenons les premières images encodées\n",
    "num_images = 10  # Par exemple, afficher 10 images encodées\n",
    "\n",
    "# Sélectionner les premières images encodées\n",
    "encoded_images = X_train_encoded[:num_images]\n",
    "\n",
    "# Affichage des images originales et des images encodées\n",
    "fig, axes = plt.subplots(2, num_images, figsize=(15, 3))\n",
    "\n",
    "for i in range(num_images):\n",
    "    # Image originale (entrée du réseau)\n",
    "    axes[0, i].imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title('Original')\n",
    "\n",
    "    # Image encodée (espace latent) - Représenter les valeurs en tant que vecteur aplati\n",
    "    latent_image = encoded_images[i].reshape(1, -1)  # Aplatir les dimensions latentes\n",
    "    axes[1, i].imshow(latent_image, cmap='gray', aspect='auto')\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title('Latent')\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "authorship_tag": "ABX9TyNp01j41IqgjHzYoYCbToA3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
